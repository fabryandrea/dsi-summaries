{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special Regression Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a subset of the predictors when creating the model. Forward Stepwise, Backward Stepwise, Forward/Backward Stepwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best subset: try every model <br>\n",
    "computationally intensive <br>\n",
    "higher chance of finding models that look good on training data <br>\n",
    "instead: stepwise selection (forward, backward, or both) <br>\n",
    "choosing among models: cross-validate, Mallows Cp, AIC, BIC, Adjusted $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization (aka Shrinkage) Methods that regularize (shrink) the coefficient estimates towards zero. Shrinkage methods can significantly reduce variance. Ridge and Lasso regressions are techniques for regularizing linear regression models. Both penalize larger coefficients with a tuning parameter (lambda). The greater the lambda value, the more regular the coefficients. As lambda approaches infinity the coefficient estimates approach 0. As lambda approaches 0, the coefficient estimates approach the estimates in a non-regularized linear regression model.\n",
    "\n",
    "Ridge Regression uses an l2 scoring model, which squares the coefficients before summing. Lasso Regression uses an l1 scoring model, which sums the absolute values of the coefficients. Lasso models have the added feature of letting feature coefficients equal 0, thus performing feature selection. Ridge regularization models will include a non-zero value for each coefficient. Neither method has proven to be universally better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regression minimizes RSS <br>\n",
    "<br>\n",
    "$\\sum_{i=1}^{n} (y_i-\\hat{y_i})^2$ <br>\n",
    "<br>\n",
    "$\\sum_{i=1}^{n} (y_i-\\beta_0-\\hat{y_i})^2$ AKA <br>\n",
    "<br>\n",
    "$\\sum_{i=1}^{n} (y_i-\\beta_0-\\sum_{i=1}^{p} (\\beta_j x_{ij})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge: <br>\n",
    "<br>\n",
    "$\\sum_{i=1}^{n} (y_i-\\beta_0-\\sum_{i=1}^{p} (\\beta_j x_{ij})^2 + \\lambda\\sum_{j=1}^{p} \\beta_j^2$ <br>\n",
    "<br>\n",
    "$\\lambda$: tuning parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso (Least Absolute Shrinkage and Selection Operator): <br>\n",
    "<br>\n",
    "$\\sum_{i=1}^{n} (y_i-\\beta_0-\\sum_{i=1}^{p} (\\beta_j x_{ij})^2 + \\lambda\\sum_{j=1}^{p} |\\beta_j|$ <br>\n",
    "<br>\n",
    "$\\lambda$: tuning parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see separate PCA document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
